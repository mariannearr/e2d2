<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="BD3-LMs">
  <meta name="twitter:description" content="BD3-LMs">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="encoder-decoder diffusion, encoder-decoder discrete diffusion, discrete, masked, diffusion, language models, E2D2, masked diffusion">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Encoder-Decoder Diffusion</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script>

</head>
<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop-wide">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Encoder-Decoder Diffusion Language Models for Efficient Training and Inference</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block"><a href="https://m-arriola.com/" target="_blank">Marianne Arriola</a><sup>*</sup>,</span>
                <span class="author-block"><a href="https://yair-schiff.github.io" target="_blank">Yair Schiff</a><sup>*</sup>,</span>
                  <span class="author-block"><a href="https://hao-pt.github.io" target="_blank">Hao Phung</a>,</span>
                  <span class="author-block"><a href="https://skylion007.github.io/" target="_blank">Aaron Gokaslan Yang</a>,</span>
                  <span class="author-block"><a href="https://www.cs.cornell.edu/~kuleshov/" target="_blank">Volodymyr Kuleshov</a></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">Department of Computer Science, Cornell University</span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>*</sup>Equal contribution; corresponding authors</span>
            </div>

            <div class="is-size-5 publication-flag" style="padding-top: 20px; padding-bottom: 20px;">
              <b>NeurIPS 2025</b>
            </div>

            <div class="link-buttons">
              <a href="https://openreview.net/forum?id=5jneOToPou&referrer=%5Bthe%20profile%20of%20Marianne%20Arriola%5D(%2Fprofile%3Fid%3D~Marianne_Arriola1)" target="_blank"
                class="button link-chip">
                <span class="icon"><i class="fas fa-file"></i></span>
                <span>Paper</span>
              </a>

              <a href="https://github.com/kuleshov-group/e2d2" target="_blank"
                class="button link-chip">
                <span class="icon"><i class="fab fa-github"></i></span>
                <span>Code</span>
              </a>

              <a href="https://huggingface.co/collections/kuleshov-group/e2d2" target="_blank"
                class="button link-chip">
                <span class="icon">
                  <img src="static/images/hf.png" alt="Hugging Face" class="hf-icon">
                </span>
                <span>HuggingFace</span>
              </a>
            </div>
                          
        </div>
      </div>
    </div>
</section>


<!-- Teaser video-->
<section class="hero teaser is-max-desktop">
  <div class="container">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">
        <span style="color: #d32f2f">Slow, <b>decoder-only</b> diffusion.</span><br>
         <span class="legend-box border-3-solid">Large decoder denoises tokens <i>and</i> caches clean context</span>
      </h2>
      <hr style="
        border: none;
        border-top: 1px solid #e0e0e0;
        width: 80%;
        margin: 0.1rem auto 1rem auto;
      ">
      <img src="static/images/bd3lm.gif" style="padding-bottom: 60px; width:80%;">

      <h2 class="subtitle has-text-centered" style="padding-top: 20px;">
        <span style="color: #2e7d32;">Fast, <b>encoder-decoder</b> diffusion.</span><br>
         <span class="legend-box border-1-solid">Large encoder caches clean context</span> <span class="legend-box border-1-dotted">Small decoder denoises tokens</span>
      </h2>
      <hr style="
        border: none;
        border-top: 1px solid #e0e0e0;
        width: 80%;
        margin: 0.1rem auto 1rem auto;
      ">
      <img src="static/images/e2d2.gif" style="padding-bottom: 20px; width:80%;">
      </div>
  </div>
</section> 

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Discrete diffusion models enable parallel token sampling for faster inference than autoregressive approaches. However, prior diffusion models use a decoder-only architecture, which requires sampling algorithms that invoke the full network at every denoising step and incur high computational cost. Our key insight is that discrete diffusion models perform two types of computation: 1) representing clean tokens and 2) denoising corrupted tokens, which enables us to use separate modules for each task. We propose an encoder-decoder architecture to accelerate discrete diffusion inference, which relies on an encoder to represent clean tokens and a lightweight decoder to iteratively refine a noised sequence. We also show that this architecture enables faster training of block diffusion models, which partition sequences into blocks for better quality and are commonly used in diffusion language model inference. We introduce a framework for <strong>E</strong>fficient <strong>E</strong>ncoder-<strong>D</strong>ecoder <strong>D</strong>iffusion (E2D2), consisting of an architecture with specialized training and sampling algorithms, and we show that E2D2 achieves superior trade-offs between generation quality and inference throughput on summarization, translation, and mathematical reasoning tasks.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- BD3-LMs -->
<style>
/* Compact, clean link chips */
      .publication-flag {
        padding-top: 8px !important; /* reduce vertical space below NeurIPS 2025 */
        padding-bottom: 8px !important; /* reduce vertical space below NeurIPS 2025 */
      }
    
      .link-buttons{
        display:flex;
        gap:.2rem;
        justify-content:center;
        align-items:center;
        flex-wrap:wrap;
        margin-top:.25rem;
      }

      .link-chip{
        /* base */
        height:auto;                 /* let padding control height */
        line-height:1.1;
        font-size:.8rem;             /* slightly smaller */
        font-weight:500;
        padding:.15rem .7rem;        /* thinner */
        border-radius:9999px;        /* pill */
        border:1px solid #e0e0e0;    /* thin outline */
        color:#fff;             /* ghost look on light bg */
        background:#111;
        box-shadow:none;
      }

      .link-chip .icon i{
        font-size:.95em;
      }

      /* HF raster icon size */
      .hf-icon{
        width:16px;
        height:16px;
        display:block;
      }

      /* Hover/focus states */
      .link-chip:hover{
        border-color:#bdbdbd;
        background:#fafafa;
      }

      .link-chip:focus{
        outline:2px solid #8ab4f8;   /* accessible focus ring */
        outline-offset:2px;
      }


      .legend-box {
        background: rgb(214, 234, 248); /* same as box_fill */
        border-radius: 6px;
        padding: 2px 4px;
        display: inline-block;
        min-width: 100px;
        text-align: center;
        color: rgb(52, 87, 112); /* same text color as your code */
        margin: 4px;
        font-size: 0.8em;
      }
      .no-border { border: none; }
      .border-3-solid { border: 3px solid #000; }
      .border-1-solid { border: 1px solid #000; }
      .border-1-dotted { border: 1px dotted #000; }

      .container.is-max-desktop {
          max-width: 50%; /* Increase the width */
          width: 50%;
      }
      .container.is-max-desktop-wide {
          max-width: 65%; /* Increase the width */
          width: 65%;
      }
      .section {
            padding-left: 5%;
            padding-right: 5%;
        }
        .hero-body {
        text-align: center; /* keep your centering intention */
      }
      
      .subtitle.has-text-centered {
          font-size: 1.3em; /* Increase font size */
      }

        p {
          font-size: 18px;
        }

        .author-block {
          font-size: 18px;
        }
      

        .figure-caption-container {
            display: flex;
            flex-direction: column; /* Captions and figure container are in a vertical stack */
            align-items: center;
            gap: 20px; /* Spacing between figure and color captions */
        }

        .figure-content-container {
            display: flex;
            flex-direction: row; /* Figure and color boxes are side by side */
            align-items: center;
            gap: 20px;
        }

        .figure-container {
            flex-shrink: 0; /* Prevents figure from shrinking */
        }

        .captions {
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: flex-start;
        }

        .caption {
            display: flex;
            align-items: center;
            margin-bottom: 10px;
            font-size: 16px;
        }
        /* .title {
          padding-bottom: 10px;
          font-size: 25px;
        } */
        /* centers the title, minimize width of container */
        /* .publication-title {
          text-align: center;
          max-width: 100%;
        } */
  .color-box {
      display: inline-block;
      width: 20px;
      height: 20px;
      margin-right: 5px;
      vertical-align: middle;
  }
  .orange { background-color: #ff9900; }
  .light-orange { background-color: #FFDAB9; }
  .blue { background-color: #30beee; }
  .dark-green { background-color: #006400; }
  .green { background-color: #228B22; }
  .light-green { background-color: #90EE90; }
</style>
<style>
  table {
    width: 50%; /* Reduce overall table width */
    margin: 10px auto;
    border-collapse: collapse;
    font-size: 18px; /* Reduce font size */
  }

  th, td {
    padding: 4px 6px; /* Reduce padding */
    border: 1px solid #ddd; /* Keep border for readability */
    text-align: center;
  }

  th {
    background-color: #f2f2f2;
  }

  .figure-caption {
    font-size: 18px; /* Reduce caption font size */
  }
</style>

<section class="section" id="SEC2">
    <div class="container is-max-desktop">
      <div class="content is-medium">
        <strong>Work in progress. Expected publication date: Oct 27.</strong>
      </div>
    </div>
</section>

<!-- 
<section class="section" id="SEC2">
  <div class="container is-max-desktop">
    <div class="content is-medium">
      <h2 class="title">BD3-LMs: Block Discrete Denoising Diffusion Language Models</h2>
      <p>We combine modeling paradigms to enjoy better likelihoods & flexible-length generation from autoregressive models, as well as fast & parallel generation from diffusion models.</p> 
      <h4 class="subtitle">Block Diffusion Likelihood</h4>
      <p>We propose a modeling framework that autoregressively models blocks of tokens and performs diffusion within each block. Our likelihood factorizes over \( B \) blocks of length \( L' \) as</p>
      <p>
        \[ \log p_\theta (\mathbf{x}) = \sum_{b=1}^B \log p_\theta (\mathbf{x}^b | \mathbf{x}^{\lt b}) \]
      </p>
      <p>Each \( p_\theta (\mathbf{x}^b | \mathbf{x}^{\lt b}) \) is modeled using discrete diffusion ELBO over a block of \( L' \) tokens. We obtain a principled learning objective \( \mathcal{L}_\text{BD}(\mathbf{x}, \theta) \) by optimizing the following likelihood bound:</p>
      <p>
        \[ \log p_\theta(\mathbf{x}) \geq \mathcal{L}_\text{BD}(\mathbf{x}, \theta) := \sum_{b=1}^{B} \mathcal{L}_{\text{diffusion}}(\mathbf{x}^b, \mathbf{x}^{\lt b}, \theta), \]
      </p>
      We model the per-block likelihood under a simple discrete diffusion parameterization (<a href="https://arxiv.org/abs/2406.07524">Sahoo et. al</a>, <a href="https://arxiv.org/abs/2406.04329">Shi et. al</a>, <a href="https://arxiv.org/abs/2406.03736">Ou et. al</a>). Our final objective becomes a sum of weighted cross-entropy terms:
      <p>
        \[ \mathcal{L}_\text{BD}(\mathbf{x}, \theta) :=  - \sum_{b=1}^{B} \mathbb{E}_{t \sim [0, 1]} \mathbb{E}_{q} \frac{1}{t} \log p_\theta(\mathbf{x}^b | \mathbf{x}_{t}^b, \mathbf{x}^{\lt b}) \]
      </p>
      <h4 class="subtitle">Efficient Training & Sampling Algorithms</h4>
      <p> Naively, we would compute the logits by applying \( \mathbf{x}_\theta^b( \mathbf{x}_t^b, \mathbf{K}^{1:b\text{-}1}, \mathbf{V}^{1:b\text{-}1}) \) in a loop \( B\) times. Instead, we only require two forward passes. The first pass precomputes keys and values \( \mathbf{K}^{1:B}, \mathbf{V}^{1:B} \) for the full sequence \( \mathbf{x}\). In the second forward pass, we compute denoised predictions for all blocks simulatenously using  \( \mathbf{x}_\theta^b( \mathbf{x}_t^b, \mathbf{K}^{1:b\text{-}1}, \mathbf{V}^{1:b\text{-}1}) \). </p>
      <p> To sample from BD3-LMs, we generate one block at a time, conditioned on previously sampled blocks. After generating a block, we cache its keys and values, similar to AR. We may use any diffusion sampling procedure \( \text{SAMPLE} ( \mathbf{x}_\theta^b, \mathbf{K}^{1:b\text{-}1}, \mathbf{V}^{1:b\text{-}1}) \) to sample from the conditional distribution \( p_\theta (\mathbf{x}_s^b | \mathbf{x}_t^b, \mathbf{x}^{ < b}) \) over \( T\) sampling steps per block.</p>
      <section class="hero is-small">
        <div class="hero-body">
        <img src="static/images/alg.png" alt="mask" style="width:80%; padding-bottom: 10px;">
        <div class="figure-caption" style="padding-bottom: 10px;">
          <i>BD3-LM training and sampling algorithms.</i>
      </div>
        </div>
      </section>
    </div>
  </div>
</section> -->

<!-- GAP -->
<!-- <section class="section" id="SEC2">
  <div class="container is-max-desktop content">
    <div class="content is-medium">
      <h2 class="title">Understanding Likelihood Gaps Between Diffusion and AR Models</h2>
      <h4 class="subtitle">Case Study: Single Token Generation</h4>
      <p>Our block diffusion parameterization is equivalent in expectation to the autoregressive NLL in the limiting case where \( L'=1 \). Surprisingly, we find a two point perplexity gap between our block diffusion model for \( L'=1 \) and AR when training both models on the LM1B dataset. We identify high training variance of the diffusion objective as responsible for the perplexity gap.</p>
      <section class="hero is-small">
        <div class="hero-body">
        <img src="static/images/bs1.png" alt="mask" style="width:80%; padding-bottom: 10px;">
        <div class="figure-caption" style="padding-bottom: 10px;">
          <i>Training under the discrete diffusion ELBO suffers from high variance.</i>
      </div>
        </div>
      </section>
      <h4 class="subtitle">Diffusion Gap from High Variance Training</h4>
        <p>Intuitively, if the sampled masking rate \( t \sim \mathcal{U}[0, 1] \) is too low, reconstructing \( \mathbf{x} \) is easy, which does not provide a useful learning signal. If we mask everything, the optimal reconstruction are the marginals of each token in the data distribution, which is easy to learn, and again not useful.</p>
        We seek to find noise schedules that minimize training variance caused by the diffusion objective and further reduce the perplexity gap.</p>
      </div>
  </div>
</section> -->

<!-- CLIPPED SCHEDULES -->
<!-- <section class="section" id="SEC2">
  <div class="container is-max-desktop content">
    <div class="content is-medium">
      <h2 class="title">Data-Driven Noise Schedules for Low-Variance Training</h2>
      <p>To avoid masking rates that cause high-variance training, we train BD3-LMs under "clipped" masking rates \( t \sim \mathcal{U}[\beta, \omega] \) for \( 0 \leq \beta, \omega \leq 1 \). By reducing the training variance, we improve likelihoods when we evaluate under uniformly sampled mask rates.</p>

      <p>As the optimal mask rates may differ depending on the block size \(L'\), we adaptively learn \( \beta, \omega \) during training. In practice, we do so using a grid search during every validation step, after 5K gradient updates, to optimize \(\min_{\beta, \omega} \text{Var}_{\mathbf{X}, t} \left[ \mathcal{L}_{\text{BD}}(\theta, \beta, \omega; \mathbf{X}) \right] \). </p>
      
      <p>Below, we show that optimizing the noise schedule per block size reduces the variance of the loss estimator and achieves the best perplexities compared to alternative schedules.</p>
    </div>

  </div>
</section> -->

<!-- RESULTS -->
<!-- <section class="section" id="SEC2">
  <div class="container is-max-desktop content">
    <div class="content is-medium">
      <h2 class="title">Results</h2>
      <h4 class="subtitle">Likelihood Evaluation</h4>
      <p>BD3-LMs achieve state-of-the-art likelihoods among diffusion models. As shown below, BD3-LMs interpolate between diffusion and autoregressive likelihoods by tuning the block length \( L' \). </p>
      <style>
        img {
          width: 80%;
        }
        table {
          border-collapse: collapse;
          width: 100%;
        }
        th, td {
          padding: 8px;
          text-align: left;
        }
        /* Top border for category headers, without bottom border */
        tr.category-row td {
          border-top: 2px solid #000;
          border-bottom: none;
        }
        /* Bold top border for the block-diffusion section */
        tr.block-diffusion-row td {
          border-top: 2px solid #000; /* Bold border */
          border-bottom: none;
        }
        /* Remove bottom borders for grouped models */
        tr.grouped-models td {
          border-bottom: none;
        }
        /* Light bottom border specifically for the last row in block-diffusion */
        tr.final-row td {
          border-bottom: 2px solid #000; /* Light border */
        }
        /* Make BD3-LMs invisible */
        .invisible {
          visibility: hidden; /* Makes text invisible but preserves space */
        }
      </style>
      <style>
        .compact-table {
            max-width: 400px;  /* Adjust width as needed */
            width: 100%;
            margin: auto;
            border-collapse: collapse;
        }
        .compact-table th, .compact-table td {
            padding: 5px 10px;  /* Reduce padding */
            text-align: left;
            white-space: nowrap; /* Prevent wrapping */
        }
        .compact-table th {
            background-color: #f2f2f2; /* Light gray header */
        }
    </style>
      <table class="compact-table">
            <center>
                <i>Test perplexities (PPL; ↓) on OWT for models trained for 262B tokens.</i>
            </center>
        </div>
        <thead>
            <tr>
                <th>Model</th>
                <th>PPL (↓)</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>AR</td>
                <td>17.54</td>
            </tr>
            <tr>
                <td>SEDD</td>
                <td>≤ 24.10</td>
            </tr>
            <tr>
                <td>MDLM</td>
                <td>≤ 22.98</td>
            </tr>
            <tr>
                <td><strong>BD3-LMs</strong> L' = 16</td>
                <td>≤ 22.27</td>
            </tr>
            <tr>
                <td><span class="invisible">BD3-LMs</span> L' = 8</td>
                <td>≤ 21.68</td>
            </tr>
            <tr>
                <td><span class="invisible">BD3-LMs</span> L' = 4</td>
                <td><strong>≤ 20.73</strong></td>
            </tr>
        </tbody>
    </table>

      <h4 class="subtitle" style="margin-top: 40px;">Arbitrary-length sequence generation</h4>
      <p>One key drawback of many existing diffusion language models is that they cannot generate full-length documents that are longer than the length of the output context chosen at training time. For example, OpenWebText contains documents up to 131K tokens, whereas discrete diffusion model SEDD (<a href="https://arxiv.org/abs/2310.16834">Lou et. al</a>) is restricted to generate 1024 tokens. Below, we show that BD3-LMs can generate variable-length documents by decoding an arbitrary number of blocks.</p>
      
      <table class="compact-table" style="margin-bottom: 40px;">
        <div class="figure-caption">
        <center>
          <i>Generation length statistics from sampling 500 documents from models trained on OWT.</i>
        </center>
      </div>
        <thead>
          <tr>
            <th></th>
            <th>Median # tokens</th>
            <th>Max # tokens</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>OWT train set</td>
            <td>717</td>
            <td>131K</td>
          </tr>
          <tr>
            <td>AR</td>
            <td>4008</td>
            <td>131K</td>
          </tr>
          <tr>
            <td>SEDD</td>
            <td>1021</td>
            <td>1024</td>
          </tr>
          <tr>
            <td><strong>BD3-LM</strong> L'=16</td>
            <td>798</td>
            <td>9982</td>
          </tr>
        </tbody>
      </table>
      
      
      <p> We assess the generation quality of BD3-LMs on variable-length sequences, comparing all methods using the same number of generation steps (NFEs). Below, we measure the generative perplexity of sampled sequences under GPT2-Large. BD3-LMs achieve the best generative perplexities compared to all previous diffusion methods.</p>
        
      <style>
        table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 20px;
        }
        th, td {
            border: 1px solid black;
            padding: 8px;
            text-align: center; /* Center align all table entries */
            vertical-align: middle;
        }
        th {
            background-color: #f2f2f2;
        }
        caption {
            text-align: center;
            font-weight: bold;
            margin-bottom: 10px;
        }
    </style>
    <p>For MDLM, we use their block-wise decoding technique (which does not feature block diffusion training as in BD3-LMs) for L=2048. We also compare to SSD-LM (<a href="https://arxiv.org/abs/2210.17432">Han et. al</a>) an alternative block-autoregressive method (also known as semi-autoregression) that performs Gaussian diffusion over word embeddings but cannot perform likelihood estimation. Our discrete approach yields samples with improved generative perplexity using an order of magnitude fewer generation steps.</p>

    </div>
  </div>
</section> -->


<!-- CONCLUSION -->
<!-- <section class="section" id="SEC2">
  <div class="container is-max-desktop content">
    <div class="content is-medium">
      <h2 class="title">Conclusion</h2>
      <p>We presented Block Discrete Diffusion Language models, a new model class that combines strength of both autoregressive and diffusion approaches while overcoming their limitations. Block diffusion overcomes key drawbacks of existing discrete diffusion models: the quality gap to AR model and their inability to generate arbitrary-length sequences or support KV caching. By doing so, BD3-LMs set a new state-of-the-art among discrete diffusion models. Our work presents a promising step forward in building powerful diffusion language models that are competitive with standard LLMs, while offering parallel token generation and improved controllability of samples.</p>
    </div>
  </div>
</section> -->




<!--BibTex citation -->
  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @inproceedings{
          arriola2025e2d2,
          title={Encoder-Decoder Diffusion Language Models for Efficient Training and Inference},
          author={Marianne Arriola and Yair Schiff and Hao Phung and Aaron Gokaslan and Volodymyr Kuleshov},
          booktitle={Advances in Neural Information Processing Systems},
          year={2025},
          url={https://openreview.net/forum?id=5jneOToPou&referrer=%5Bthe%20profile%20of%20Marianne%20Arriola%5D(%2Fprofile%3Fid%3D~Marianne_Arriola1)}
        }
      </code></pre>
    </div>
</section> -->
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>